
[[Y-HandBook]]

## Введение

Есть три сходных между собой понятия, три сестры: интерполяция, аппроксимация и регрессия.  

У них общая цель: из семейства функций выбрать ту, которая обладает определенным свойством.  
  
![](https://habrastorage.org/r/w1560/webt/qg/pu/yw/qgpuyw14gedgfx2naxzd7wvhiw4.png)  

**Интерполяция** — способ выбрать из семейства функций ту, которая проходит через заданные точки. Часто функцию затем используют для вычисления в промежуточных точках. Например, мы вручную задаем цвет нескольким точкам и хотим чтобы цвета остальных точек образовали плавные переходы между заданными. Или задаем ключевые кадры анимации и хотим плавные переходы между ними. Классические примеры: интерполяция полиномами Лагранжа, сплайн-интерполяция, многомерная интерполяция (билинейная, трилинейная, методом ближайшего соседа и т.д). Есть также родственное понятие экстраполяции — предсказание поведения функции вне интервала. Например, предсказание курса доллара на основании предыдущих колебаний — экстраполяция.

> Интерполяция — это построение функции, которая проходит через известные точки и позволяет найти значения между ними.

### Интерполяция полиномами Лагранжа

**Формула полинома Лагранжа:**
$$

P_n(x)=\sum_{i=0}^{n} y_i \, L_i(x),

\quad \text{где} \quad

L_i(x)=\prod_{\substack{j=0 \\ j\neq i}}^{n}\frac{x-x_j}{x_i-x_j}.

$$
### Пояснение переменных и смысла формулы

* Заданные точки интерполяции: $(x_0,y_0), (x_1,y_1), \dots, (x_n,y_n)$ — известные пары координат;
* $P_n(x)$ — интерполяционный полином степени не выше $n$, проходящий **точно** через все заданные точки;
* $L_i(x)$ — базисный полином Лагранжа, определённый так, что он равен $1$ в точке $x_i$ и $0$ во всех остальных $x_j$ при $j\neq i$;
* Сумма
$$P_n(x)=\sum_{i=0}^{n} y_i \, L_i(x)$$
«взвешивает» каждый базисный полином значением $y_i$, обеспечивая выполнение условий $P_n(x_i)=y_i$ для всех $i$.

  
![](https://habrastorage.org/r/w1560/webt/p-/k5/mi/p-k5mikdujjfdsmwpbsvphmkd4c.png)

**Аппроксимация** — способ выбрать из семейства «простых» функций приближение для «сложной» функции на отрезке, при этом ошибка не должна превышать определенного предела. Аппроксимацию используют, когда нужно получить функцию, похожую на данную, но более удобную для вычислений и манипуляций (дифференцирования, интегрирования и т.п). При оптимизации критических участков кода часто используют аппроксимацию: если значение функции вычисляется много раз в секунду и не нужна абсолютная точность, то можно обойтись более простым аппроксимантом с меньшей «ценой» вычисления. Классические примеры включают ряд Тейлора на отрезке, аппроксимацию ортогональными многочленами, аппроксимацию Паде, аппроксимацию синуса Бхаскара и т.п.

> **Аппроксимация** — это способ подобрать более простую функцию, близко приближающуюся к заданной на участке так, чтобы ошибка не превышала заданный предел и вычисления были легче.


  
![](https://habrastorage.org/r/w1560/webt/mj/gl/d6/mjgld6nvylinet9ud1irmsahjq4.png)

**Регрессия** — способ выбрать из семейства функций ту, которая минимизирует функцию потерь. Последняя характеризует насколько сильно пробная функция отклоняется от значений в заданных точках. Если точки получены в эксперименте, они неизбежно содержат ошибку измерений, шум, поэтому разумнее требовать, чтобы функция передавала общую тенденцию, а не точно проходила через все точки. В каком-то смысле регрессия — это «интерполирующая аппроксимация»: мы хотим провести кривую как можно ближе к точкам и при этом сохранить ее максимально простой чтобы уловить общую тенденцию. За баланс между этими противоречивыми желаниями как-раз отвечает функция потерь (в английской литературе «loss function» или «cost function»).

> **Регрессия** — это способ выбрать такую функцию из семейства кандидатов, которая **минимально отклоняется от данных (считая ошибки и шум)** и передаёт **общую тенденцию**, а не точно проходит через все точки.

Мы начнём с использования линейных моделей для решения задачи регрессии. Простейшим примером постановки задачи линейной регрессии является **метод наименьших квадратов** (*Ordinary least squares*).

Пусть у нас задан датасет $(X, y)$, где  $y = (y_i)_{i=1}^N \in \mathbb{R}^N$ — вектор значений целевой переменной, а  $X = (x_i)_{i=1}^N \in \mathbb{R}^{N \times D},\; x_i \in \mathbb{R}^D$ — матрица объекты-признаки, в которой $i$-я строка — это вектор признаков $i$-го объекта выборки. Мы хотим моделировать зависимость $y_i$ от $x_i$ как линейную функцию со свободным членом. Общий вид такой функции из $\mathbb{R}^D$ в $\mathbb{R}$ выглядит следующим образом:

$$
f_w(x_i) = \langle w, x_i \rangle + w_0
$$

Свободный член $w_0$ часто опускают, потому что такого же результата можно добиться, добавив ко всем $x_i$ признак, тождественно равный единице; тогда роль свободного члена будет играть соответствующий ему вес:

$$
(x_{i1};\dots;x_{iD}) \cdot
\begin{pmatrix}
w_1 \\
\vdots \\
w_D
\end{pmatrix}
*
w_0
=
(1;x_{i1};\dots;x_{iD}) \cdot
\begin{pmatrix}
w_0 \\
w_1 \\
\vdots \\
w_D
\end{pmatrix}
$$

Поскольку это сильно упрощает запись, в дальнейшем мы будем считать, что это уже сделано и зависимость имеет вид просто

$$f_w(x_i) = \langle w, x_i \rangle$$
---

## Сведение к задаче оптимизации

Мы хотим, чтобы на нашем датасете (то есть на парах $(x_i, y_i)$ из обучающей выборки) функция $f_w$ как можно лучше приближала нашу зависимость.

Для того, чтобы чётко сформулировать задачу, нам осталось только одно: на математическом языке выразить желание «приблизить $f_w(x)$ к $y$». Говоря простым языком, мы должны научиться измерять качество модели и минимизировать её ошибку, как-то меняя обучаемые параметры. В нашем примере обучаемые параметры — это веса $w$. Функция, оценивающая то, как часто модель ошибается, традиционно называется **функцией потерь**, **функционалом качества** или просто **лоссом** (*loss function*). Важно, чтобы её было легко оптимизировать: скажем, гладкая функция потерь — это хорошо, а кусочно постоянная — просто ужасно.

Функции потерь бывают разными. От их выбора зависит то, насколько задачу в дальнейшем легко решать, и то, в каком смысле у нас получится приблизить предсказание модели к целевым значениям. Интуитивно понятно, что для нашей текущей задачи нам нужно взять вектор $y$ и вектор предсказаний модели и как-то сравнить, насколько они похожи. Так как эти вектора «живут» в одном векторном пространстве, расстояние между ними вполне может быть функцией потерь. Более того, положительная непрерывная функция от этого расстояния тоже подойдёт в качестве функции потерь. При этом способов задать расстояние между векторами тоже довольно много. От всего этого разнообразия глаза разбегаются, но мы обязательно поговорим про это позже.

Сейчас давайте в качестве лосса возьмём квадрат $L_2$-нормы вектора разницы предсказаний модели и $y$. Во-первых, как мы увидим дальше, так задачу будет нетрудно решить, а во-вторых, у этого лосса есть ещё несколько дополнительных свойств:

* $L_2$-норма разницы — это евклидово расстояние  $\lVert y - f_w(x) \rVert_2$  
  между вектором таргетов и вектором ответов модели, то есть мы их приближаем в смысле самого простого и понятного «расстояния».
* Как мы увидим в разделе про вероятностные модели, с точки зрения статистики это соответствует гипотезе о том, что наши данные состоят из линейного «сигнала» и нормально распределенного «шума».

Так вот, наша функция потерь выглядит так:

$$
L(f, X, y) = \lVert y - f(X) \rVert_2^2
= \lVert y - Xw \rVert_2^2
= \sum_{i=1}^N (y_i - \langle x_i, w \rangle)^2
$$

Такой функционал ошибки не очень хорош для сравнения поведения моделей на выборках разного размера. Представьте, что вы хотите понять, насколько качество модели на тестовой выборке из $2500$ объектов хуже, чем на обучающей из $5000$ объектов. Вы измерили $L_2$-норму ошибки и получили в одном случае $300$, а в другом $500$. Эти числа не очень интерпретируемы. Гораздо лучше посмотреть на среднеквадратичное отклонение:

$$
L(f, X, y) = \frac{1}{N} \sum_{i=1}^N (y_i - \langle x_i, w \rangle)^2
$$

По этой метрике на тестовой выборке получаем $0{,}12$, а на обучающей $0{,}1$.

Функция потерь  $\frac{1}{N} \sum_{i=1}^N (y_i - \langle x_i, w \rangle)^2$  называется **Mean Squared Error**, **MSE** или **среднеквадратическим отклонением**. Разница с $L_2$-нормой чисто косметическая, на алгоритм решения задачи она не влияет:

$$
\mathrm{MSE}(f, X, y) = \frac{1}{N} \lVert y - Xw \rVert_2^2
$$

В самом широком смысле, функции работают с объектами множеств: берут какой-то входящий объект из одного множества и выдают на выходе соответствующий ему объект из другого. Если мы имеем дело с отображением, которое на вход принимает функции, а на выходе выдаёт число, то такое отображение называют **функционалом**. Если вы посмотрите на нашу функцию потерь, то увидите, что это именно функционал. Для каждой конкретной линейной функции, которую задают веса $w_i$, мы получаем число, которое оценивает, насколько точно эта функция приближает наши значения $y$. Чем меньше это число, тем точнее наше решение, значит для того, чтобы найти лучшую модель, этот функционал нам надо минимизировать по $w$:

$$
\lVert y - Xw \rVert_2^2 \;\xrightarrow[w]{}\; \min
$$

Эту задачу можно решать разными способами. В этом параграфе мы сначала решим эту задачу аналитически, а потом приближенно. Сравнение двух этих решений позволит нам проиллюстрировать преимущества того подхода, которому посвящена эта книга. На наш взгляд, это самый простой способ «на пальцах» показать суть машинного обучения.

## МНК: геометрический подход

![[Y-HandBook/Infographics/МНК-геометрический подход/1.png]]

Пусть $x^{(1)}, \ldots, x^{(D)}$ — столбцы матрицы $X$, то есть столбцы признаков. Тогда

$$
Xw = w_1 x^{(1)} + \ldots + w_D x^{(D)},
$$

и задачу регрессии можно сформулировать следующим образом: найти линейную комбинацию столбцов $x^{(1)}, \ldots, x^{(D)}$, которая наилучшим способом приближает столбец $y$ по евклидовой норме — то есть найти проекцию вектора $y$ на подпространство, образованное векторами $x^{(1)}, \ldots, x^{(D)}$.

Разложим $y = y_{\parallel} + y_{\perp}$, где $y_{\parallel} = Xw$ — та самая проекция, а $y_{\perp}$ — ортогональная составляющая, то есть $y_{\perp} = y - Xw \perp x^{(1)}, \ldots, x^{(D)}$. Как это можно выразить в матричном виде? Оказывается, очень просто:

$$
X^T(y - Xw) = 0
$$

В самом деле, каждый элемент столбца $X^T(y - Xw)$ — это скалярное произведение строки $X^T$ (=столбца $X$ = одного из $x^{(i)}$) на $y - Xw$. Из уравнения $X^T(y - Xw) = 0$ уже очень легко выразить $w$:

$$
w = (X^T X)^{-1} X^T y
$$