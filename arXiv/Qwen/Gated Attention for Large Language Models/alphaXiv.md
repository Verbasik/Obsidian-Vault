# Управляемое внимание для больших языковых моделей: нелинейность, разреженность и отсутствие "воронки внимания"

## Введение

Механизмы внимания стали краеугольным камнем больших языковых моделей (БЯМ), позволяя им обрабатывать и понимать контекстуальные связи во входных последовательностях. В то время как исследователи постоянно совершенствуют архитектуру transformer, лежащую в основе этих моделей, одно конкретное улучшение — механизмы управления (gating mechanisms) — было широко принято, но остается недостаточно изученным.

![[Qwen/Gated Attention for Large Language Models/Infographics/1.jpeg]]
> Рисунок 1: Различные позиции управления в слое внимания (слева), их влияние на перплексию и производительность MMLU (в центре) и стабильность обучения (справа). Обратите внимание, что G1, расположенный после Scaled Dot Product Attention (SDPA), показывает наиболее значительные улучшения.

Механизмы управления, возникшие в рекуррентных нейронных сетях, таких как LSTM и GRU, вводят контролируемый поток информации в нейронные сети. ***В контексте БЯМ на основе transformer управление действует как динамический фильтр, позволяя модели выборочно выделять или подавлять информацию.*** Несмотря на широкое распространение, конкретный вклад управления в производительность БЯМ остается малоизученным, и их эффекты часто смешиваются с другими архитектурными модификациями.

В этой статье представлен систематический анализ механизмов управления в слоях внимания transformer, изучающий более 30 вариантов softmax внимания, дополненного управлением. Благодаря обширным экспериментам как с 15B моделями Mixture-of-Experts, так и с 1,7B плотными моделями, обученными на триллионах токенов, исследование показывает, что механизмы управления предлагают преимущества, выходящие далеко за рамки того, что было известно ранее.

## Понимание механизмов управления (Gating Mechanisms)

Управление в контексте нейронных сетей относится к механизму, который контролирует поток информации через сеть. В своей основной форме вентиль (gate) — это слой нейронной сети, который выводит значения от 0 до 1, которые затем используются для модуляции потока информации посредством поэлементного умножения.

В традиционной архитектуре transformer вычисление внимания можно представить как:

$$
\begin{aligned}
\mathrm{Attention}(Q, K, V)
&= \operatorname{softmax}\!\left( \frac{QK^{\mathsf T}}{\sqrt{d_k}} \right) V
\end{aligned}
$$

Управление вводит дополнительные параметры и операции в этот процесс. Исследователи изучили различные позиции, в которых можно ввести управление:

1. G1: После вывода Scaled Dot Product Attention (SDPA);
2. G2: В проекции значения (V);
3. G3: В проекции ключа (K);
4. G4: В проекции запроса (Q);
5. G5: После финального плотного выходного слоя.

Кроме того, они исследовали различные варианты управления:

* Поэлементное vs. покомпонентное управление (headwise gating);
* Параметры, специфичные для компонента vs. параметры, общие для всех компонентов;
* Аддитивные vs. мультипликативные формы.

Ключевым отличием их подхода была изоляция влияния управления от других архитектурных изменений, что позволило лучше понять конкретный вклад управления.

## Экспериментальная методология

Исследователи использовали систематический и всесторонний экспериментальный подход для оценки эффектов механизмов управления:

* Размеры моделей: эксперименты проводились как на 15B моделях Mixture-of-Experts (MoE), так и на 1.7B плотных моделях, чтобы убедиться, что результаты согласуются в разных масштабах;
* Данные для обучения: модели были обучены на массивном наборе данных из 3.5 триллионов токенов.
Варианты гейтинга: Было протестировано более 30 различных вариантов гейтинг-дополненного softmax внимания;
* Оценка: модели оценивались на стандартных бенчмарках, включая Hellaswag, MMLU, GSM8k, HumanEval, C-eval и CMMLU, а также с помощью измерений перплексии на выделенных тестовых наборах.

Этот методичный подход позволил исследователям систематически определить, какие конфигурации гейтинга обеспечивают наиболее значительные преимущества и почему они работают.

## Ключевые выводы

Исследование привело к нескольким важным выводам о механизмах гейтинга в слоях внимания трансформера:

1. Оптимальная позиция гейтинга: гейтинг, применяемый после выхода Scaled Dot Product Attention (SDPA) (позиция G1), неизменно давал наилучшие результаты как для больших, так и для малых моделей. Эта конфигурация снизила перплексию на -0.265 и улучшила производительность MMLU на +2.03 пункта по сравнению с базовым уровнем.

2. Два ключевых механизма: эффективность гейтинга проистекает из двух основных механизмов:

* Нелинейность: гейтинг вводит важные нелинейные преобразования после низкорангового отображения в softmax внимании.
* Разреженность: зависящие от запроса разреженные оценки гейтинга модулируют выход SDPA, создавая более сфокусированный шаблон внимания.

3. Преимущества обучения: модели с гейтингом показали улучшенную стабильность обучения, что позволило использовать более высокие скорости обучения и более быструю сходимость. Как показано на рисунке 1 (правая панель), кривая потерь обучения для модели с гейтингом (синяя линия) стабильно более гладкая, чем у базовой модели (серая линия).

4. Смягчение "воронки внимания": возможно, наиболее значимым является то, что гейтинг помогает смягчить проблему "воронки внимания", когда модели непропорционально обращают внимание на начальные токены в последовательности, ограничивая эффективное использование контекста.

## Проблема "воронки внимания"

Проблема "воронки внимания" является существенным ограничением в стандартных моделях-трансформерах. Она относится к явлению, когда непропорциональное количество внимания уделяется первым нескольким токенам последовательности, особенно в более глубоких слоях модели.

![[2.jpeg]]

> Рисунок 2: Сравнение оценок внимания к первому токену по слоям в базовой модели (вверху) и в модели с гейтинговым вниманием (внизу). Базовая модель показывает высокое внимание к первому токену (в среднем 0.467), в то время как модель с гейтингом значительно снижает его (в среднем 0.048).

Как показано на рисунке 2, в базовой модели оценки внимания к первому токену значительно увеличиваются в более глубоких слоях, со средней оценкой 0.467 по всем слоям. Это создает "воронку внимания", в которой вычислительные ресурсы тратятся впустую на начальные токены, а не распределяются по всему контексту.

В отличие от этого, модель с гейтинговым вниманием резко снижает акцент на первом токене, со средней оценкой внимания всего 0.048. Карты внимания (правые панели) ясно показывают эту разницу: в слое 21 базовой модели 83% внимания направлено на первый токен, в то время как модель с гейтингом снижает это значение до всего 4%.

Это смягчение воронки внимания имеет глубокие последствия для способности LLM эффективно обрабатывать длинные контексты. Снижая непропорциональный акцент на начальных токенах, гейтинговое внимание позволяет модели поддерживать осведомленность обо всем контексте, улучшая производительность в задачах, требующих понимания на большом расстоянии.

## Разреженность в гейтинговом внимании

Одним из ключевых механизмов, с помощью которого гейтинг улучшает производительность LLM, является введение разреженности в шаблоны внимания. Гейтинг создает избирательный акцент на релевантных токенах, подавляя внимание к менее важным.

![[3.jpeg]]
> Рисунок 3: Распределение оценок вентиля для вывода SDPA, демонстрирующее сильно скошенное распределение со средним значением 0,116, что указывает на высокую разреженность.

Распределение оценок вентиля, как показано на рисунке 3, сильно смещено в сторону меньших значений со средним значением 0,116. Это указывает на то, что механизм вентиляции активно подавляет многие связи, усиливая лишь некоторые.

Этот эффект разреженности можно количественно оценить, изучив коэффициент разреженности до и после вентиляции:

![[4.jpeg]]
> Рисунок 4: Коэффициент разреженности до и после вентиляции для разных слоев и порогов. Вентиляция значительно увеличивает разреженность, особенно в ранних слоях.

Исследователи обнаружили, что для порога 1e-2 средний коэффициент разреженности увеличивается с 0,03 до вентиляции до 0,44 после вентиляции. Это означает, что почти половина выходов внимания эффективно подавляется механизмом вентиляции. Даже при более строгом пороге в 1e-3 коэффициент разреженности увеличивается с 0,003 до 0,126 после вентиляции.

Эта разреженность по сути создает адаптивную, зависящую от входных данных обрезку весов внимания, позволяя модели сосредоточить вычислительные ресурсы на наиболее релевантных соединениях.

## Стабильность обучения и производительность

Помимо влияния на шаблоны внимания, вентиляция обеспечивает значительные преимущества для обучения и производительности модели:

![[5.jpeg]]

> Рисунок 5: Сравнение средних значений скрытых состояний по слоям для различных конфигураций модели. Вентиляция значительно снижает величины активации, способствуя стабильности обучения.

Механизм вентиляции помогает регулировать величины скрытого состояния по всей сети. Как показано на рисунке 5, средние значения скрытого состояния до вентиляции (синяя пунктирная линия) резко возрастают в более глубоких слоях, достигая значений до 4,0. После вентиляции (оранжевая пунктирная линия) эти значения снижаются в среднем до 0,05, что сопоставимо с 0,04 базовой модели.

Эта регуляция величин активации способствует более стабильному обучению, позволяя:

1. **Более высокие скорости обучения**: модели с вентилями можно обучать с более высокими скоростями обучения без расхождения.
2. **Более быстрая сходимость**: потери при обучении уменьшаются более последовательно и быстро.
3. **Улучшенная обобщающая способность**: модели демонстрируют лучшую производительность в эталонных задачах.

Исследователи наблюдали стабильные улучшения по целому ряду эталонных тестов, особенно заметные улучшения в задачах, требующих сложных рассуждений и понимания длинного контекста.

## Последствия для проектирования БЯМ

Результаты имеют несколько важных последствий для проектирования и разработки будущих БЯМ:

1. **Архитектурная эффективность**: механизмы вентиляции представляют собой относительно легкую модификацию стандартных механизмов внимания, но при этом обеспечивают существенное повышение производительности.

2. **Расширение контекстного окна**: благодаря смягчению проблемы поглощения внимания, вентилируемое внимание позволяет более эффективно обрабатывать более длинные контексты, что потенциально позволяет использовать более крупные контекстные окна без пропорционального увеличения вычислительных требований.

3. **Эффективность обучения**: улучшенная стабильность позволяет проводить более агрессивную оптимизацию, что потенциально сокращает время обучения и необходимые вычислительные ресурсы.

4. **Масштабирование модели**: преимущества вентиляции, по-видимому, масштабируются с размером модели, что позволяет предположить, что вентиляция может стать все более важной по мере дальнейшего роста моделей.

Исследователи также наблюдали интересные закономерности в том, как различные варианты вентиляции влияют на значения вентиля в разных слоях:

![[6.jpeg]]
> Рисунок 6: Статистика значений вентиля по слоям для различных вариантов вентиляции, показывающая, как поведение вентиляции изменяется в зависимости от глубины модели.

Эти закономерности предполагают, что разные слои используют стробирование по-разному, причём некоторые слои применяют более агрессивную фильтрацию, чем другие. Это соответствие иерархической природе обучения представлений в глубоких сетях подчёркивает, как стробирование адаптируется к конкретным потребностям различных этапов обработки.

## Заключение

Это исследование предоставляет всесторонний анализ механизмов gating'a в слоях внимания LLM, раскрывая их многогранные преимущества. За счёт привнесения нелинейности и разреженности в механизмы внимания,  gating значительно улучшает производительность модели, стабильность обучения и использование контекста.

Обнаружение того, что  gating эффективно смягчает проблему "воронки внимания" (attention sink), представляет собой особенно ценный вклад, поскольку решает фундаментальное ограничение моделей на основе трансформеров. Этот вывод имеет последствия не только для производительности модели, но и для эффективности обработки длинных контекстов.

Систематический подход, использованный исследователями, с тестированием многочисленных вариантов gating'a для моделей разных размеров, предоставляет убедительные доказательства преимуществ gating'a. Их вывод о том, что gating, специфичное для выходной головы SDPA (G1), даёт наиболее значительные улучшения, предлагает чёткое направление для будущего проектирования архитектуры LLM.

По мере того как LLM продолжают развиваться, включение оптимизированных механизмов gating'a может стать стандартной практикой, позволяя создавать более эффективные модели, способные справляться со всё более сложными задачами и более длинными контекстами. 

GitHub: https://github.com/qiuzh20/gated_attention/tree/main

[[Gated Attention for Large Language Models]]